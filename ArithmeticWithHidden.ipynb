{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "id": "OzGh9ahKF17h"
   },
   "outputs": [],
   "source": [
    "num_digits = 3\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabd151a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fabd151a",
    "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuCc6jF5F8hK",
    "outputId": "e42dddaa-dacd-471e-b7d0-48f17004dd87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "fff6c129-8820-4b3c-8caa-b43710370098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 10, 4, 2, 11], '12+42=')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"12 + 42 =\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daa90f31",
    "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('815+387=', '1202')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(num_digits = 3):\n",
    "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    a_str = \"\".join([str(x) for x in a_list])\n",
    "    b_str = \"\".join([str(x) for x in b_list])\n",
    "    sum_int = a_int + b_int\n",
    "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6e861d2",
    "outputId": "7b639165-7809-4f2d-a067-f21547c293b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('981+018=', '999'),\n",
       " ('122+426=', '548'),\n",
       " ('050+584=', '634'),\n",
       " ('795+358=', '1153')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(num_digits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee85050",
    "outputId": "69def39c-084c-4074-f296-908925038748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57600, 6400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6227f5b-df1d-46d1-b8b8-e9e8003dbb25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modify the TransformerEncoder class to return hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98a0dd08-5091-40c6-82e4-7ec1097ddba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.transformer import *\n",
    "from torch.nn.modules.transformer import _generate_square_subsequent_mask, _get_seq_len, _get_clones, _get_activation_fn, _detect_is_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09a6d94-4f4f-4ad0-8c24-119e35dd9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "from typing import Any, Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.normalization import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "925cee6c-9949-4cdf-8a5b-e8b3d295f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithHidden(nn.Module):\n",
    "    r\"\"\"A transformer model.\n",
    "\n",
    "    .. note::\n",
    "        See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
    "        for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
    "        transformer layers.\n",
    "\n",
    "    User is able to modify the attributes as needed. The architecture\n",
    "    is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\n",
    "    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n",
    "    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\n",
    "    Processing Systems, pages 6000-6010.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
    "        nhead: the number of heads in the multiheadattention models (default=8).\n",
    "        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
    "        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of encoder/decoder intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        custom_encoder: custom encoder (default=None).\n",
    "        custom_decoder: custom decoder (default=None).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before\n",
    "            other attention and feedforward operations, otherwise after. Default: ``False`` (after).\n",
    "        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n",
    "            bias. Default: ``True``.\n",
    "\n",
    "    Examples::\n",
    "        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "        >>> src = torch.rand((10, 32, 512))\n",
    "        >>> tgt = torch.rand((20, 32, 512))\n",
    "        >>> out = transformer_model(src, tgt)\n",
    "\n",
    "    Note: A full example to apply nn.Transformer module for the word language model is available in\n",
    "    https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "        custom_encoder: Optional[Any] = None,\n",
    "        custom_decoder: Optional[Any] = None,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        batch_first: bool = False,\n",
    "        norm_first: bool = False,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(\n",
    "                d_model,\n",
    "                nhead,\n",
    "                dim_feedforward,\n",
    "                dropout,\n",
    "                activation,\n",
    "                layer_norm_eps,\n",
    "                batch_first,\n",
    "                norm_first,\n",
    "                bias,\n",
    "                **factory_kwargs,\n",
    "            )\n",
    "            encoder_norm = LayerNorm(\n",
    "                d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs\n",
    "            )\n",
    "            self.encoder = TransformerEncoderWithHidden( ### MODIFIED ###\n",
    "                encoder_layer, num_encoder_layers, encoder_norm\n",
    "            )\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(\n",
    "                d_model,\n",
    "                nhead,\n",
    "                dim_feedforward,\n",
    "                dropout,\n",
    "                activation,\n",
    "                layer_norm_eps,\n",
    "                batch_first,\n",
    "                norm_first,\n",
    "                bias,\n",
    "                **factory_kwargs,\n",
    "            )\n",
    "            decoder_norm = LayerNorm(\n",
    "                d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs\n",
    "            )\n",
    "            self.decoder = TransformerDecoder(\n",
    "                decoder_layer, num_decoder_layers, decoder_norm\n",
    "            )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        tgt: Tensor,\n",
    "        src_mask: Optional[Tensor] = None,\n",
    "        tgt_mask: Optional[Tensor] = None,\n",
    "        memory_mask: Optional[Tensor] = None,\n",
    "        src_key_padding_mask: Optional[Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[Tensor] = None,\n",
    "        src_is_causal: Optional[bool] = None,\n",
    "        tgt_is_causal: Optional[bool] = None,\n",
    "        memory_is_causal: bool = False,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Take in and process masked source/target sequences.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a ``True`` value are\n",
    "            not allowed to participate in the attention,\n",
    "            which is the opposite of the definition for :attr:`attn_mask`\n",
    "            in :func:`torch.nn.functional.scaled_dot_product_attention`.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            src_mask: the additive mask for the src sequence (optional).\n",
    "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "            memory_mask: the additive mask for the encoder output (optional).\n",
    "            src_key_padding_mask: the Tensor mask for src keys per batch (optional).\n",
    "            tgt_key_padding_mask: the Tensor mask for tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the Tensor mask for memory keys per batch (optional).\n",
    "            src_is_causal: If specified, applies a causal mask as ``src_mask``.\n",
    "                Default: ``None``; try to detect a causal mask.\n",
    "                Warning:\n",
    "                ``src_is_causal`` provides a hint that ``src_mask`` is\n",
    "                the causal mask. Providing incorrect hints can result in\n",
    "                incorrect execution, including forward and backward\n",
    "                compatibility.\n",
    "            tgt_is_causal: If specified, applies a causal mask as ``tgt_mask``.\n",
    "                Default: ``None``; try to detect a causal mask.\n",
    "                Warning:\n",
    "                ``tgt_is_causal`` provides a hint that ``tgt_mask`` is\n",
    "                the causal mask. Providing incorrect hints can result in\n",
    "                incorrect execution, including forward and backward\n",
    "                compatibility.\n",
    "            memory_is_causal: If specified, applies a causal mask as\n",
    "                ``memory_mask``.\n",
    "                Default: ``False``.\n",
    "                Warning:\n",
    "                ``memory_is_causal`` provides a hint that\n",
    "                ``memory_mask`` is the causal mask. Providing incorrect\n",
    "                hints can result in incorrect execution, including\n",
    "                forward and backward compatibility.\n",
    "\n",
    "        Shape:\n",
    "            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n",
    "              `(N, S, E)` if `batch_first=True`.\n",
    "            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "            - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\n",
    "            - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\n",
    "            - memory_mask: :math:`(T, S)`.\n",
    "            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n",
    "            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "\n",
    "            Note: [src/tgt/memory]_mask ensures that position :math:`i` is allowed to attend the unmasked\n",
    "            positions. If a BoolTensor is provided, positions with ``True``\n",
    "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "            is provided, it will be added to the attention weight.\n",
    "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "            the attention. If a BoolTensor is provided, the positions with the\n",
    "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "\n",
    "            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "\n",
    "            Note: Due to the multi-head attention architecture in the transformer model,\n",
    "            the output sequence length of a transformer is same as the input sequence\n",
    "            (i.e. target) length of the decoder.\n",
    "\n",
    "            where :math:`S` is the source sequence length, :math:`T` is the target sequence length, :math:`N` is the\n",
    "            batch size, :math:`E` is the feature number\n",
    "\n",
    "        Examples:\n",
    "            >>> # xdoctest: +SKIP\n",
    "            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \"\"\"\n",
    "        is_batched = src.dim() == 3\n",
    "        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n",
    "            raise RuntimeError(\n",
    "                \"the feature number of src and tgt must be equal to d_model\"\n",
    "            )\n",
    "\n",
    "        memory, hidden = self.encoder( ### MODIFIED ###\n",
    "            src,\n",
    "            mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            is_causal=src_is_causal,\n",
    "        )\n",
    "        output = self.decoder(\n",
    "            tgt,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "            tgt_is_causal=tgt_is_causal,\n",
    "            memory_is_causal=memory_is_causal,\n",
    "        )\n",
    "        return output, hidden ### MODIFIED ###\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(\n",
    "        sz: int,\n",
    "        device: Optional[torch.device] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Generate a square causal mask for the sequence.\n",
    "\n",
    "        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        return _generate_square_subsequent_mask(sz, dtype=dtype, device=device)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c1dd361-209e-4b57-b6f1-4f09395340f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderWithHidden(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers.\n",
    "\n",
    "    .. note::\n",
    "        See `this tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
    "        for an in depth discussion of the performant building blocks PyTorch offers for building your own\n",
    "        transformer layers.\n",
    "\n",
    "    Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.\n",
    "\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "        enable_nested_tensor: if True, input will automatically convert to nested tensor\n",
    "            (and convert back on output). This will improve the overall performance of\n",
    "            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = [\"norm\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layer: \"TransformerEncoderLayer\",\n",
    "        num_layers: int,\n",
    "        norm: Optional[Module] = None,\n",
    "        enable_nested_tensor: bool = True,\n",
    "        mask_check: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        # this attribute saves the value providedat object construction\n",
    "        self.enable_nested_tensor = enable_nested_tensor\n",
    "        # this attribute controls whether nested tensors are used\n",
    "        self.use_nested_tensor = enable_nested_tensor\n",
    "        self.mask_check = mask_check\n",
    "\n",
    "        enc_layer = \"encoder_layer\"\n",
    "        why_not_sparsity_fast_path = \"\"\n",
    "        if not isinstance(encoder_layer, torch.nn.TransformerEncoderLayer):\n",
    "            why_not_sparsity_fast_path = f\"{enc_layer} was not TransformerEncoderLayer\"\n",
    "        elif encoder_layer.norm_first:\n",
    "            why_not_sparsity_fast_path = f\"{enc_layer}.norm_first was True\"\n",
    "        elif not encoder_layer.self_attn.batch_first:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                f\"{enc_layer}.self_attn.batch_first was not True\"\n",
    "                + \"(use batch_first for better inference performance)\"\n",
    "            )\n",
    "        elif not encoder_layer.self_attn._qkv_same_embed_dim:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                f\"{enc_layer}.self_attn._qkv_same_embed_dim was not True\"\n",
    "            )\n",
    "        elif encoder_layer.self_attn.in_proj_bias is None:\n",
    "            why_not_sparsity_fast_path = f\"{enc_layer}.self_attn was passed bias=False\"\n",
    "        elif not encoder_layer.activation_relu_or_gelu:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                f\"{enc_layer}.activation_relu_or_gelu was not True\"\n",
    "            )\n",
    "        elif not (encoder_layer.norm1.eps == encoder_layer.norm2.eps):\n",
    "            why_not_sparsity_fast_path = (\n",
    "                f\"{enc_layer}.norm1.eps was not equal to {enc_layer}.norm2.eps\"\n",
    "            )\n",
    "        elif encoder_layer.self_attn.num_heads % 2 == 1:\n",
    "            why_not_sparsity_fast_path = f\"{enc_layer}.self_attn.num_heads is odd\"\n",
    "\n",
    "        if enable_nested_tensor and why_not_sparsity_fast_path:\n",
    "            warnings.warn(\n",
    "                f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\"\n",
    "            )\n",
    "            self.use_nested_tensor = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        src_key_padding_mask: Optional[Tensor] = None,\n",
    "        is_causal: Optional[bool] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "            is_causal: If specified, applies a causal mask as ``mask``.\n",
    "                Default: ``None``; try to detect a causal mask.\n",
    "                Warning:\n",
    "                ``is_causal`` provides a hint that ``mask`` is the\n",
    "                causal mask. Providing incorrect hints can result in\n",
    "                incorrect execution, including forward and backward\n",
    "                compatibility.\n",
    "\n",
    "        Shape:\n",
    "            see the docs in :class:`~torch.nn.Transformer`.\n",
    "        \"\"\"\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(mask),\n",
    "            other_name=\"mask\",\n",
    "            target_type=src.dtype,\n",
    "        )\n",
    "\n",
    "        mask = F._canonical_mask(\n",
    "            mask=mask,\n",
    "            mask_name=\"mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        src_key_padding_mask_for_layers = src_key_padding_mask\n",
    "        why_not_sparsity_fast_path = \"\"\n",
    "        str_first_layer = \"self.layers[0]\"\n",
    "        batch_first = first_layer.self_attn.batch_first\n",
    "        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n",
    "\n",
    "        if not is_fastpath_enabled:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                \"torch.backends.mha.get_fastpath_enabled() was not True\"\n",
    "            )\n",
    "        elif not hasattr(self, \"use_nested_tensor\"):\n",
    "            why_not_sparsity_fast_path = \"use_nested_tensor attribute not present\"\n",
    "        elif not self.use_nested_tensor:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                \"self.use_nested_tensor (set in init) was not True\"\n",
    "            )\n",
    "        elif first_layer.training:\n",
    "            why_not_sparsity_fast_path = f\"{str_first_layer} was in training mode\"\n",
    "        elif not src.dim() == 3:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "            )\n",
    "        elif src_key_padding_mask is None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask was None\"\n",
    "        elif (\n",
    "            (not hasattr(self, \"mask_check\")) or self.mask_check\n",
    "        ) and not torch._nested_tensor_from_mask_left_aligned(\n",
    "            src, src_key_padding_mask.logical_not()\n",
    "        ):\n",
    "            why_not_sparsity_fast_path = \"mask_check enabled, and src and src_key_padding_mask was not left aligned\"\n",
    "        elif output.is_nested:\n",
    "            why_not_sparsity_fast_path = \"NestedTensor input is not supported\"\n",
    "        elif mask is not None:\n",
    "            why_not_sparsity_fast_path = (\n",
    "                \"src_key_padding_mask and mask were both supplied\"\n",
    "            )\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_sparsity_fast_path:\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                first_layer.self_attn.in_proj_weight,\n",
    "                first_layer.self_attn.in_proj_bias,\n",
    "                first_layer.self_attn.out_proj.weight,\n",
    "                first_layer.self_attn.out_proj.bias,\n",
    "                first_layer.norm1.weight,\n",
    "                first_layer.norm1.bias,\n",
    "                first_layer.norm2.weight,\n",
    "                first_layer.norm2.bias,\n",
    "                first_layer.linear1.weight,\n",
    "                first_layer.linear1.bias,\n",
    "                first_layer.linear2.weight,\n",
    "                first_layer.linear2.bias,\n",
    "            )\n",
    "            _supported_device_type = [\n",
    "                \"cpu\",\n",
    "                \"cuda\",\n",
    "                torch.utils.backend_registration._privateuse1_backend_name,\n",
    "            ]\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif src.device.type not in _supported_device_type:\n",
    "                why_not_sparsity_fast_path = (\n",
    "                    f\"src device is neither one of {_supported_device_type}\"\n",
    "                )\n",
    "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = (\n",
    "                    \"grad is enabled and at least one of query or the \"\n",
    "                    \"input/output projection weights or biases requires_grad\"\n",
    "                )\n",
    "\n",
    "            if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):\n",
    "                convert_to_nested = True\n",
    "                output = torch._nested_tensor_from_mask(\n",
    "                    output, src_key_padding_mask.logical_not(), mask_check=False\n",
    "                )\n",
    "                src_key_padding_mask_for_layers = None\n",
    "\n",
    "        seq_len = _get_seq_len(src, batch_first)\n",
    "        is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n",
    "\n",
    "        hidden = [] ### ADDED ###\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(\n",
    "                output,\n",
    "                src_mask=mask,\n",
    "                is_causal=is_causal,\n",
    "                src_key_padding_mask=src_key_padding_mask_for_layers,\n",
    "            )\n",
    "            hidden.append(output.detach()) ### ADDED ###\n",
    "\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0.0, src.size())\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, hidden ### MODIFIED ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91674239",
   "metadata": {
    "id": "91674239"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eb278ab",
   "metadata": {
    "id": "4eb278ab"
   },
   "outputs": [],
   "source": [
    "class TransformerModelWithHidden(TransformerWithHidden):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModelWithHidden, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc, hidden = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return output_dec, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3129/854229317.py:76: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModelWithHidden(\n",
       "  (encoder): TransformerEncoderWithHidden(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (input_emb): Embedding(14, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModelWithHidden(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6PmJSo95N4C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6PmJSo95N4C",
    "outputId": "f0177342-43b6-46c6-b079-aab40db326d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 668942\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35d113",
   "metadata": {
    "id": "2e35d113"
   },
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f2f06e0",
   "metadata": {
    "id": "8f2f06e0"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
    "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
    "    # (prompt_length, batch_size * num_samples)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
    "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
    "        if mode == \"greedy\":\n",
    "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        else: # mode == \"sampling\"\n",
    "            logits /= temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d76d1b19",
    "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 10,  3, 11,  2,  2,  2,  2,  2]]), '2+3=22222')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00954ddc",
   "metadata": {
    "id": "00954ddc"
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c84beab",
    "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "264f9227",
   "metadata": {
    "id": "264f9227"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i, batch_size):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "\n",
    "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
    "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
    "\n",
    "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
    "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
    "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
    "\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, prompt_length, answers_length, prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91e281ad",
    "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '067+163=', '230')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
    "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "KfmcSdPwp3K6",
   "metadata": {
    "id": "KfmcSdPwp3K6"
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cfcd10a",
   "metadata": {
    "id": "1cfcd10a"
   },
   "outputs": [],
   "source": [
    "def evaluate(batch_size = batch_size):\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size)\n",
    "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac335b05",
    "outputId": "1355d497-45f0-440a-90b8-30a7f2818091"
   },
   "outputs": [],
   "source": [
    "# evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 5: Train the model, classical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827e567",
   "metadata": {
    "id": "b827e567"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b140ba3",
   "metadata": {
    "id": "5b140ba3"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 8e-4\n",
    "\n",
    "reporting_per_epoch = 5\n",
    "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
    "assert(log_interval % batch_size == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3638a75d",
   "metadata": {
    "id": "3638a75d"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
    "            model.zero_grad()\n",
    "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens)\n",
    "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens)\n",
    "            target_answers = target_answers.view(-1)\n",
    "            loss = F.cross_entropy(output_answers, target_answers)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2a8490",
    "outputId": "492562cc-d243-4314-ab56-d17d41c070ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   600/ 3600 batches | ms/batch 53.63 | loss  0.09 | perplexity     1.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m loss = F.cross_entropy(output_answers, target_answers)\n\u001b[32m     24\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m total_loss += loss.item()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % log_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py:243\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    230\u001b[39m     beta1, beta2 = cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    232\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    233\u001b[39m         group,\n\u001b[32m    234\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m         state_steps,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py:875\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    873\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m875\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py:477\u001b[39m, in \u001b[36m_single_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    475\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m         denom = \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d9d440",
    "outputId": "b876390d-4396-4a95-e600-e2230317dbec"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc312c-c757-4e8e-9c76-e13ca91be38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arithmetic.pt\", 'wb') as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7007280-f134-4205-aeb2-59c997cf50be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79cc73-6aff-49a8-aa2f-4a955bbc41fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ac58a-d5c3-469f-8f58-d8f627e6ff4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7f01c-45eb-4343-b662-6cb76ff76568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
