{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "from transformer_with_hidden import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce8f10-d10a-4414-9483-984ab13271c9",
   "metadata": {},
   "source": [
    "## Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d5add4-16bd-4c0c-9b11-23e68bf694ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lens:\n",
    "    \"\"\"\n",
    "    Implementation of the logit and tuned lens method for visualizing intermediate layer predictions\n",
    "    in transformer models.\n",
    "    \n",
    "    The logit and tuned lens allows us to decode hidden states at each layer of a transformer\n",
    "    using the unembedding matrix to observe how predictions evolve through the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the LogitLens.\n",
    "        \n",
    "        Args:\n",
    "            model: The transformer model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.unembed_weight = self.model.decoder.weight # weight of the Linear module in the implementation of the course\n",
    "    \n",
    "    def logit_lens(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the standard logit lens to a hidden state.\n",
    "        \n",
    "        Args:\n",
    "            hidden_state: Hidden state from an intermediate layer\n",
    "            \n",
    "        Returns:\n",
    "            Logits obtained by projecting the hidden state through the unembedding matrix\n",
    "        \"\"\"\n",
    "        # Project through the unembedding matrix\n",
    "        logits = F.linear(hidden_state, self.unembed_weight)\n",
    "        return logits\n",
    "    \n",
    "    def tuned_lens(self, \n",
    "                   hidden_state: torch.Tensor, \n",
    "                   translator: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the tuned logit lens to a hidden state using a learned translator.\n",
    "        \n",
    "        Args:\n",
    "            hidden_state: Hidden state from an intermediate layer\n",
    "            translator: A learned affine transformation module\n",
    "            \n",
    "        Returns:\n",
    "            Logits obtained by applying the translator and then projecting through\n",
    "            the unembedding matrix\n",
    "        \"\"\"\n",
    "        # Apply the translator\n",
    "        translated_state = translator(hidden_state)\n",
    "        \n",
    "        # Project through the unembedding matrix\n",
    "        logits = F.linear(translated_state, self.unembed_weight)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e1a738-3ba2-4200-921c-8470906b7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModule(nn.Module):\n",
    "    \"\"\"A learned affine transformation for the tuned lens.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the translator module.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: Size of the hidden state\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply the translator to a hidden state.\"\"\"\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede9a792-bc22-4a55-a583-d1d33a39b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithLens(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for a transformer model that captures intermediate hidden states\n",
    "    and applies the lens method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 transformer_model: nn.Module, \n",
    "                 num_layers: int,\n",
    "                 hidden_size: int,\n",
    "                 \n",
    "                 use_tuned_lens: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the wrapper.\n",
    "        \n",
    "        Args:\n",
    "            transformer_model: The transformer model to wrap\n",
    "            num_layers: Number of layers in the transformer\n",
    "            hidden_size: Dimensionality of the hidden states\n",
    "            use_tuned_lens: Whether to use the tuned lens with learned translators\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_tuned_lens = use_tuned_lens\n",
    "            \n",
    "        # Initialize the logit lens\n",
    "        self.logit_lens = Lens(transformer_model)\n",
    "        \n",
    "        # Initialize one translator per layer for tuned lens if needed\n",
    "        if use_tuned_lens:\n",
    "            self.translators = nn.ModuleList([\n",
    "                TranslatorModule(hidden_size) for _ in range(num_layers)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, \n",
    "                inputs: Dict[str, torch.Tensor]\n",
    "                ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Forward pass that includes logit lens visualizations.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensors for the transformer model\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing:\n",
    "                - 'output': The original model output\n",
    "                - 'logit_lens_outputs': List of logit lens outputs for each layer\n",
    "                - 'tuned_lens_outputs': List of tuned lens outputs for each layer (if enabled)\n",
    "        \"\"\"\n",
    "        # Get the original model output and the hidden states\n",
    "        outputs, hidden_states = self.transformer(inputs)\n",
    "                \n",
    "        # Apply logit lens to each hidden state\n",
    "        logit_lens_outputs = [\n",
    "            self.logit_lens.logit_lens(hidden_state)\n",
    "            for hidden_state in hidden_states\n",
    "        ]\n",
    "        \n",
    "        # Apply tuned lens if enabled\n",
    "        tuned_lens_outputs = None\n",
    "        if self.use_tuned_lens:\n",
    "            tuned_lens_outputs = [\n",
    "                self.logit_lens.tuned_lens(hidden_states[i], self.translators[i])\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        \n",
    "        return {\n",
    "            'output': outputs,\n",
    "            'logit_lens_outputs': logit_lens_outputs,\n",
    "            'tuned_lens_outputs': tuned_lens_outputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1dedc37-28e8-4500-b908-2dc8abe332e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(logits: torch.Tensor, \n",
    "                          tokenizer, \n",
    "                          top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Visualize top-k predictions from logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: The logits tensor\n",
    "        tokenizer: The tokenizer for converting token IDs to strings\n",
    "        top_k: Number of top predictions to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (token, probability) tuples for the top-k predictions\n",
    "    \"\"\"\n",
    "    # Get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    values, indices = torch.topk(probs, top_k)\n",
    "    indices, values = indices.flatten().tolist(), values.flatten().tolist()\n",
    "    \n",
    "    # Convert to list of (token, probability) tuples\n",
    "    predictions = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        probability = values[i]\n",
    "        predictions.append((token, probability))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281b606a-3320-4598-965a-9a1a0c823ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_logit_lens(model, tokenizer, input_text: str):\n",
    "    \"\"\"\n",
    "    Demonstrate the logit lens by visualizing predictions for all intermediate layers.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        tokenizer: The tokenizer\n",
    "        input_text: The input text to process\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = torch.tensor(tokenizer.encode(input_text)).view((-1,1))\n",
    "    \n",
    "    # Create the wrapper\n",
    "    wrapper = TransformerWithLens(model, num_layers=len(model.encoder.layers), hidden_size=model.ninp)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = wrapper(inputs)\n",
    "    \n",
    "    # Get logit lens outputs\n",
    "    logit_lens_outputs = outputs['logit_lens_outputs']\n",
    "\n",
    "    # Visualize predictions for all layers and the final prediction\n",
    "    for i, layer_outputs in enumerate(logit_lens_outputs):\n",
    "        predictions = visualize_predictions(layer_outputs[-1, :, :].squeeze(1), tokenizer)\n",
    "        print(f\"Layer {i+1} predictions:\")\n",
    "        for token, prob in predictions:\n",
    "            print(f\"  {token}: {prob:.4f}\")\n",
    "            \n",
    "    predictions = visualize_predictions(outputs['output'][-1, :, :].squeeze(1), tokenizer)\n",
    "    print(f\"Final prediction:\")\n",
    "    for token, prob in predictions:\n",
    "        print(f\"  {token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5582afc7-f7ae-4046-afc7-76bdbc2de6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_tuned_lens(model, tokenizer, input_text: str):\n",
    "    \"\"\"\n",
    "    Demonstrate the tuned lens by visualizing predictions for all intermediate layers.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        tokenizer: The tokenizer\n",
    "        input_text: The input text to process\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = torch.tensor(tokenizer.encode(input_text)).view((-1,1))\n",
    "    \n",
    "    # Create the wrapper, attention pour les tuned lens ça ne maarchera pas comme ça, \n",
    "    # il faudra en entraîner un\n",
    "    wrapper = TransformerWithLens(model, num_layers=len(model.encoder.layers), hidden_size=model.ninp, use_tuned_lens=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = wrapper(inputs)\n",
    "    \n",
    "    # Get tuned lens outputs\n",
    "    tuned_lens_outputs = outputs['tuned_lens_outputs']\n",
    "\n",
    "    # Visualize predictions for all layers and the final prediction\n",
    "    for i, layer_outputs in enumerate(tuned_lens_outputs):\n",
    "        predictions = visualize_predictions(layer_outputs[-1, :, :].squeeze(1), tokenizer)\n",
    "        print(f\"Layer {i+1} predictions:\")\n",
    "        for token, prob in predictions:\n",
    "            print(f\"  {token}: {prob:.4f}\")\n",
    "            \n",
    "    predictions = visualize_predictions(outputs['output'][-1, :, :].squeeze(1), tokenizer)\n",
    "    print(f\"Final prediction:\")\n",
    "    for token, prob in predictions:\n",
    "        print(f\"  {token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73b106-4451-4535-a4ba-602f9586c8f9",
   "metadata": {},
   "source": [
    "## Utility functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "id": "QuCc6jF5F8hK"
   },
   "outputs": [],
   "source": [
    "tokenizer = character_level_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767319e2-d8ed-4bf9-bf05-1995a04dccae",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7911d949-8ba9-471b-af0c-f696f2cd7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9fbe12-e3e1-4c43-b4ce-0ee155605f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModelWithHidden(\n",
       "  (encoder): TransformerEncoderWithHidden(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (input_emb): Embedding(14, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('arithmetic.pt', weights_only=False, map_location='cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca29e105-1d35-4f9b-9a60-e8fbdd75055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 predictions:\n",
      "  [PAD]: 0.2270\n",
      "  7: 0.1506\n",
      "  8: 0.0996\n",
      "  [EOS]: 0.0832\n",
      "  9: 0.0672\n",
      "Layer 2 predictions:\n",
      "  7: 0.3376\n",
      "  8: 0.1695\n",
      "  [PAD]: 0.0855\n",
      "  6: 0.0848\n",
      "  1: 0.0807\n",
      "Layer 3 predictions:\n",
      "  7: 0.2038\n",
      "  8: 0.1282\n",
      "  6: 0.1246\n",
      "  9: 0.1180\n",
      "  1: 0.1065\n",
      "Layer 4 predictions:\n",
      "  7: 0.1626\n",
      "  8: 0.1480\n",
      "  6: 0.1340\n",
      "  9: 0.1031\n",
      "  3: 0.0791\n",
      "Layer 5 predictions:\n",
      "  4: 0.2591\n",
      "  3: 0.1762\n",
      "  2: 0.0974\n",
      "  6: 0.0768\n",
      "  5: 0.0687\n",
      "Layer 6 predictions:\n",
      "  3: 0.4060\n",
      "  2: 0.2325\n",
      "  4: 0.1261\n",
      "  1: 0.0725\n",
      "  9: 0.0508\n",
      "Layer 7 predictions:\n",
      "  2: 0.6465\n",
      "  3: 0.3199\n",
      "  1: 0.0130\n",
      "  4: 0.0059\n",
      "  9: 0.0046\n",
      "Layer 8 predictions:\n",
      "  2: 0.4921\n",
      "  3: 0.4891\n",
      "  1: 0.0089\n",
      "  4: 0.0070\n",
      "  9: 0.0008\n",
      "Final prediction:\n",
      "  3: 0.4946\n",
      "  2: 0.4900\n",
      "  1: 0.0082\n",
      "  4: 0.0055\n",
      "  9: 0.0005\n"
     ]
    }
   ],
   "source": [
    "demonstrate_logit_lens(model, tokenizer, input_text='123+182=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21ca7216-697f-49b2-ae72-957632c195a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 predictions:\n",
      "  7: 0.1528\n",
      "  =: 0.1143\n",
      "  6: 0.1049\n",
      "  5: 0.1047\n",
      "  8: 0.0851\n",
      "Layer 2 predictions:\n",
      "  1: 0.1070\n",
      "  7: 0.1036\n",
      "  +: 0.0983\n",
      "  0: 0.0941\n",
      "  2: 0.0925\n",
      "Layer 3 predictions:\n",
      "  [PAD]: 0.1261\n",
      "  5: 0.1106\n",
      "  6: 0.1099\n",
      "  4: 0.0989\n",
      "  0: 0.0835\n",
      "Layer 4 predictions:\n",
      "  9: 0.1484\n",
      "  8: 0.1175\n",
      "  7: 0.1028\n",
      "  6: 0.0923\n",
      "  5: 0.0858\n",
      "Layer 5 predictions:\n",
      "  =: 0.1911\n",
      "  +: 0.1415\n",
      "  [EOS]: 0.0878\n",
      "  7: 0.0740\n",
      "  9: 0.0735\n",
      "Layer 6 predictions:\n",
      "  9: 0.1438\n",
      "  [PAD]: 0.1322\n",
      "  =: 0.1047\n",
      "  8: 0.1047\n",
      "  +: 0.0992\n",
      "Layer 7 predictions:\n",
      "  =: 0.1707\n",
      "  +: 0.1541\n",
      "  3: 0.1203\n",
      "  [EOS]: 0.1080\n",
      "  2: 0.0884\n",
      "Layer 8 predictions:\n",
      "  [EOS]: 0.2078\n",
      "  2: 0.1442\n",
      "  1: 0.1148\n",
      "  0: 0.0867\n",
      "  3: 0.0844\n",
      "Final prediction:\n",
      "  3: 0.4946\n",
      "  2: 0.4900\n",
      "  1: 0.0082\n",
      "  4: 0.0055\n",
      "  9: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# untrained attempt of tuned lens, just to wheck it works\n",
    "demonstrate_tuned_lens(model, tokenizer, input_text='123+182=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3715a8cb-cfbe-47fc-92b4-24feb53e1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = TransformerWithLens(model, num_layers=len(model.encoder.layers), hidden_size=model.ninp, use_tuned_lens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ad07043-0450-4605-800c-3277928e57b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-7): 8 x TranslatorModule(\n",
       "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.translators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
