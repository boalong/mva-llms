{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80517dbc",
      "metadata": {
        "id": "80517dbc"
      },
      "source": [
        "# Teach an LLM to do additions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aaca18f",
      "metadata": {
        "id": "0aaca18f"
      },
      "source": [
        "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
        "* the tokenizer\n",
        "* the positional embedding\n",
        "\n",
        "Both the model and the dataset are fixed.\n",
        "\n",
        "You are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "ae993bb9",
      "metadata": {
        "id": "ae993bb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "OzGh9ahKF17h",
      "metadata": {
        "id": "OzGh9ahKF17h"
      },
      "outputs": [],
      "source": [
        "number_bits = 4 # numeber of additions digits\n",
        "\n",
        "#we can only change the tokenizer and the positional embedding\n",
        "\n",
        "dataset_size = 1_024_000\n",
        "train_proportion = 0.9\n",
        "\n",
        "log_interval = 200\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c054bed",
      "metadata": {
        "id": "6c054bed"
      },
      "source": [
        "## Step 1: Construct a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "t6aC9uNeIR6C",
      "metadata": {
        "id": "t6aC9uNeIR6C"
      },
      "outputs": [],
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BMvT0B-MGBnY",
      "metadata": {
        "id": "BMvT0B-MGBnY"
      },
      "source": [
        "### Baseline: character-level tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "g2QiF-otFur3",
      "metadata": {
        "id": "g2QiF-otFur3"
      },
      "outputs": [],
      "source": [
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "p6Z8FMtGT4s8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6Z8FMtGT4s8",
        "outputId": "6b484352-b118-494d-ed55-339d7b1e124a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 2, 10, 3, 4, 11], ['1', '2', '+', '3', '4', '='], '12+34=')"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"12 + 34 =\"\n",
        "tokenizer = character_level_tokenizer()\n",
        "input = tokenizer.encode(prompt)\n",
        "input, [tokenizer.decode([x]) for x in input], tokenizer.decode(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "QuCc6jF5F8hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCc6jF5F8hK",
        "outputId": "80899b64-966c-42d4-d7b8-722c94bd21cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491af297",
      "metadata": {
        "id": "491af297"
      },
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "daa90f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "1026a48a-9d19-43c7-a4c9-ca808060ed3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('445+385=', '830')"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sample_datapoint(number_bits = 3):\n",
        "    \"\"\"\n",
        "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
        "    \"\"\"\n",
        "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    sum_int = a_int + b_int\n",
        "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "b6e861d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "3cc932ac-c2eb-4147-9d17-3ba73e8d6908"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('2332+2988=', '5320'),\n",
              " ('1685+7404=', '9089'),\n",
              " ('689+1876=', '2565'),\n",
              " ('5891+1707=', '7598')]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(number_bits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "fee85050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "7d96a5e0-ac7a-4c6a-e35b-a9b4a1bfb5e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(921600, 102400)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37200598",
      "metadata": {
        "id": "37200598"
      },
      "source": [
        "## Step 3: Construct a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd7d2eb",
      "metadata": {
        "id": "0fd7d2eb"
      },
      "source": [
        "### Basline: the classical Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "91674239",
      "metadata": {
        "id": "91674239"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8296ceb2",
      "metadata": {
        "id": "8296ceb2"
      },
      "source": [
        "# Implement your positional embedding here!\n",
        "\n",
        "You can do anything. Some ideas:\n",
        "* RoPE\n",
        "* (randomised) FIRE\n",
        "* Abacus\n",
        "\n",
        "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
        "(length_prompts, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "4eb278ab",
      "metadata": {
        "id": "4eb278ab"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEmbedding(ninp, dropout)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=ninp, nhead=nhead, dim_feedforward=nhid, dropout=dropout)\n",
        "            for _ in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [sequence_length, batch_size] assumed\n",
        "        \n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(src.device)\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        \n",
        "        hidden_states = []\n",
        "        # Pass through each encoder layer and store the hidden states.\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask=mask)\n",
        "            hidden_states.append(src)\n",
        "        \n",
        "        output = self.decoder(src)\n",
        "        # Return both the final log probabilities and the list of hidden states.\n",
        "        return F.log_softmax(output, dim=-1), hidden_states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "42f9d1ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42f9d1ee",
        "outputId": "700ad852-94d3-4ec7-f1bb-f186562015a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30e093a",
      "metadata": {
        "id": "a30e093a"
      },
      "source": [
        "Please do not change these parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "1d568cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "3507ee8b-3360-4573-d829-6a2c5e3677ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEmbedding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-7): 8 x TransformerEncoderLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.5, inplace=False)\n",
              "      (dropout2): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "a7147b61",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "a9aebe97",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "8f2f06e0",
      "metadata": {
        "id": "8f2f06e0"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5):\n",
        "    input_tensor = prompts # (length_prompts, batch_size)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n",
        "        last_output = output[-1,:,:] # (batch_size, ntokens)\n",
        "        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n",
        "        input_tensor = torch.cat((input_tensor, token), 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "d76d1b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "a184977c-28fe-4269-939b-9013c6053fcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2, 10,  3, 11,  4, 10,  4, 10,  4]]), '2+3=4+4+4')"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() # this disables the droppout so it is deterministic and basically is the inference step\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output.tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "00954ddc",
      "metadata": {
        "id": "00954ddc"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list])\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "2c84beab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "160396f1-d743-462c-cfd8-c0e6b3a1f6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "264f9227",
      "metadata": {
        "id": "264f9227"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
        "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
        "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
        "    padded_answers, length_answers = pad(answers, \"answers\")\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, length_prompts, length_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "91e281ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "f84b5c3e-b273-415c-91b2-5cd30d085bee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10, 256]), torch.Size([6, 256]), 10, 5)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
        "X.shape, Y.shape, length_prompts, length_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113e1fd1",
      "metadata": {
        "id": "113e1fd1"
      },
      "source": [
        "## Step 4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "1cfcd10a",
      "metadata": {
        "id": "1cfcd10a"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
        "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
        "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
        "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
        "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
        "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ac335b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "a266bcd3-a1e3-47c4-81d0-8e1ae8971251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c54061a",
      "metadata": {
        "id": "4c54061a"
      },
      "source": [
        "## Step 4: Train the model (transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "3638a75d",
      "metadata": {
        "id": "3638a75d"
      },
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
        "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
        "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
        "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
        "        model.zero_grad()\n",
        "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
        "\n",
        "        ####### ADDS EXPLANATION TO THE FOLLOWING 3 LINES OF CODE DURING TRAINING SINCE IT IS UNUSUAL\n",
        "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
        "        target_answers = target_answers.view(-1)\n",
        "        loss = F.cross_entropy(output_answers, target_answers)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def train():\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_epoch()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic_modified_tuned_lens.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "004f844b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "4e2a8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "1cb8a2a6-3c0f-4e84-b1bc-7c2ce16052fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/ 3600 batches | ms/batch 196.16 | loss  1.83 | perplexity     6.22\n",
            "|   400/ 3600 batches | ms/batch 194.48 | loss  1.59 | perplexity     4.90\n",
            "|   600/ 3600 batches | ms/batch 727.08 | loss  1.50 | perplexity     4.48\n",
            "|   800/ 3600 batches | ms/batch 1013.60 | loss  1.43 | perplexity     4.19\n",
            "|  1000/ 3600 batches | ms/batch 1022.93 | loss  1.39 | perplexity     4.03\n",
            "|  1200/ 3600 batches | ms/batch 889.71 | loss  1.37 | perplexity     3.93\n",
            "|  1400/ 3600 batches | ms/batch 1046.77 | loss  1.35 | perplexity     3.86\n",
            "|  1600/ 3600 batches | ms/batch 829.09 | loss  1.33 | perplexity     3.79\n",
            "|  1800/ 3600 batches | ms/batch 988.81 | loss  1.32 | perplexity     3.76\n",
            "|  2000/ 3600 batches | ms/batch 1041.29 | loss  1.31 | perplexity     3.72\n",
            "|  2200/ 3600 batches | ms/batch 864.41 | loss  1.31 | perplexity     3.70\n",
            "|  2400/ 3600 batches | ms/batch 202.46 | loss  1.30 | perplexity     3.67\n",
            "|  2600/ 3600 batches | ms/batch 736.40 | loss  1.30 | perplexity     3.65\n",
            "|  2800/ 3600 batches | ms/batch 987.65 | loss  1.29 | perplexity     3.63\n",
            "|  3000/ 3600 batches | ms/batch 1122.15 | loss  1.28 | perplexity     3.61\n",
            "|  3200/ 3600 batches | ms/batch 799.56 | loss  1.28 | perplexity     3.60\n",
            "|  3400/ 3600 batches | ms/batch 1026.57 | loss  1.28 | perplexity     3.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 3316.01s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/ 3600 batches | ms/batch 244.82 | loss  1.28 | perplexity     3.58\n",
            "|   400/ 3600 batches | ms/batch 233.51 | loss  1.27 | perplexity     3.55\n",
            "|   600/ 3600 batches | ms/batch 234.77 | loss  1.26 | perplexity     3.53\n",
            "|   800/ 3600 batches | ms/batch 236.30 | loss  1.26 | perplexity     3.53\n",
            "|  1000/ 3600 batches | ms/batch 234.37 | loss  1.26 | perplexity     3.52\n",
            "|  1200/ 3600 batches | ms/batch 248.89 | loss  1.26 | perplexity     3.51\n",
            "|  1400/ 3600 batches | ms/batch 209.74 | loss  1.25 | perplexity     3.51\n",
            "|  1600/ 3600 batches | ms/batch 245.23 | loss  1.25 | perplexity     3.49\n",
            "|  1800/ 3600 batches | ms/batch 190.68 | loss  1.25 | perplexity     3.48\n",
            "|  2000/ 3600 batches | ms/batch 196.34 | loss  1.25 | perplexity     3.47\n",
            "|  2200/ 3600 batches | ms/batch 196.08 | loss  1.24 | perplexity     3.47\n",
            "|  2400/ 3600 batches | ms/batch 195.80 | loss  1.24 | perplexity     3.46\n",
            "|  2600/ 3600 batches | ms/batch 194.88 | loss  1.24 | perplexity     3.46\n",
            "|  2800/ 3600 batches | ms/batch 195.24 | loss  1.24 | perplexity     3.46\n",
            "|  3000/ 3600 batches | ms/batch 199.59 | loss  1.24 | perplexity     3.44\n",
            "|  3200/ 3600 batches | ms/batch 220.83 | loss  1.24 | perplexity     3.44\n",
            "|  3400/ 3600 batches | ms/batch 209.36 | loss  1.24 | perplexity     3.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 884.26s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/ 3600 batches | ms/batch 198.12 | loss  1.24 | perplexity     3.44\n",
            "|   400/ 3600 batches | ms/batch 212.55 | loss  1.23 | perplexity     3.41\n",
            "|   600/ 3600 batches | ms/batch 203.44 | loss  1.22 | perplexity     3.40\n",
            "|   800/ 3600 batches | ms/batch 198.49 | loss  1.22 | perplexity     3.37\n",
            "|  1000/ 3600 batches | ms/batch 198.57 | loss  1.20 | perplexity     3.32\n",
            "|  1200/ 3600 batches | ms/batch 196.78 | loss  1.18 | perplexity     3.27\n",
            "|  1400/ 3600 batches | ms/batch 196.59 | loss  1.17 | perplexity     3.22\n",
            "|  1600/ 3600 batches | ms/batch 196.62 | loss  1.15 | perplexity     3.16\n",
            "|  1800/ 3600 batches | ms/batch 196.09 | loss  1.14 | perplexity     3.12\n",
            "|  2000/ 3600 batches | ms/batch 196.46 | loss  1.12 | perplexity     3.07\n",
            "|  2200/ 3600 batches | ms/batch 197.49 | loss  1.11 | perplexity     3.05\n",
            "|  2400/ 3600 batches | ms/batch 197.88 | loss  1.10 | perplexity     2.99\n",
            "|  2600/ 3600 batches | ms/batch 197.48 | loss  1.09 | perplexity     2.97\n",
            "|  2800/ 3600 batches | ms/batch 196.55 | loss  1.09 | perplexity     2.97\n",
            "|  3000/ 3600 batches | ms/batch 196.32 | loss  1.08 | perplexity     2.93\n",
            "|  3200/ 3600 batches | ms/batch 197.55 | loss  1.07 | perplexity     2.90\n",
            "|  3400/ 3600 batches | ms/batch 196.71 | loss  1.06 | perplexity     2.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 828.18s | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/ 3600 batches | ms/batch 209.12 | loss  1.06 | perplexity     2.89\n",
            "|   400/ 3600 batches | ms/batch 207.24 | loss  1.05 | perplexity     2.85\n",
            "|   600/ 3600 batches | ms/batch 206.35 | loss  1.05 | perplexity     2.84\n",
            "|   800/ 3600 batches | ms/batch 206.33 | loss  1.04 | perplexity     2.83\n",
            "|  1000/ 3600 batches | ms/batch 205.54 | loss  1.04 | perplexity     2.82\n",
            "|  1200/ 3600 batches | ms/batch 208.49 | loss  1.04 | perplexity     2.83\n",
            "|  1400/ 3600 batches | ms/batch 207.96 | loss  1.03 | perplexity     2.81\n",
            "|  1600/ 3600 batches | ms/batch 207.04 | loss  1.03 | perplexity     2.79\n",
            "|  1800/ 3600 batches | ms/batch 208.07 | loss  1.03 | perplexity     2.79\n",
            "|  2000/ 3600 batches | ms/batch 207.65 | loss  1.02 | perplexity     2.78\n",
            "|  2200/ 3600 batches | ms/batch 207.29 | loss  1.02 | perplexity     2.78\n",
            "|  2400/ 3600 batches | ms/batch 207.05 | loss  1.02 | perplexity     2.78\n",
            "|  2600/ 3600 batches | ms/batch 206.20 | loss  1.03 | perplexity     2.79\n",
            "|  2800/ 3600 batches | ms/batch 207.79 | loss  1.02 | perplexity     2.76\n",
            "|  3000/ 3600 batches | ms/batch 207.72 | loss  1.02 | perplexity     2.76\n",
            "|  3200/ 3600 batches | ms/batch 200.99 | loss  1.02 | perplexity     2.76\n",
            "|  3400/ 3600 batches | ms/batch 215.67 | loss  1.01 | perplexity     2.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 857.28s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   200/ 3600 batches | ms/batch 210.92 | loss  1.02 | perplexity     2.76\n",
            "|   400/ 3600 batches | ms/batch 241.59 | loss  1.01 | perplexity     2.74\n",
            "|   600/ 3600 batches | ms/batch 277.19 | loss  1.00 | perplexity     2.72\n",
            "|   800/ 3600 batches | ms/batch 238.28 | loss  1.00 | perplexity     2.73\n",
            "|  1000/ 3600 batches | ms/batch 235.04 | loss  1.00 | perplexity     2.72\n",
            "|  1200/ 3600 batches | ms/batch 240.95 | loss  1.00 | perplexity     2.73\n",
            "|  1400/ 3600 batches | ms/batch 235.74 | loss  1.00 | perplexity     2.72\n",
            "|  1600/ 3600 batches | ms/batch 237.48 | loss  1.00 | perplexity     2.71\n",
            "|  1800/ 3600 batches | ms/batch 232.91 | loss  0.99 | perplexity     2.70\n",
            "|  2000/ 3600 batches | ms/batch 234.39 | loss  1.00 | perplexity     2.70\n",
            "|  2200/ 3600 batches | ms/batch 236.69 | loss  1.00 | perplexity     2.71\n",
            "|  2400/ 3600 batches | ms/batch 238.21 | loss  1.00 | perplexity     2.72\n",
            "|  2600/ 3600 batches | ms/batch 237.27 | loss  1.00 | perplexity     2.71\n",
            "|  2800/ 3600 batches | ms/batch 238.61 | loss  1.00 | perplexity     2.71\n",
            "|  3000/ 3600 batches | ms/batch 235.30 | loss  0.99 | perplexity     2.70\n",
            "|  3200/ 3600 batches | ms/batch 234.52 | loss  0.99 | perplexity     2.69\n",
            "|  3400/ 3600 batches | ms/batch 234.82 | loss  0.99 | perplexity     2.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 1043.86s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "56d9d440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "701bafa0-4e87-4d72-8800-d6133e68ac31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6066+7574=13599\t actual result: 13640\n",
            "7620+1382=8999\t actual result: 9002\n",
            "626+55=111\t actual result: 681\n",
            "2749+5275=8099\t actual result: 8024\n",
            "1651+5711=7499\t actual result: 7362\n",
            "6783+8232=15999\t actual result: 15015\n",
            "3631+7341=10999\t actual result: 10972\n",
            "4181+9333=13499\t actual result: 13514\n",
            "8843+1751=10599\t actual result: 10594\n",
            "3817+8048=11899\t actual result: 11865\n",
            "9416+1214=10699\t actual result: 10630\n",
            "3375+9296=12699\t actual result: 12671\n",
            "9923+1941=11899\t actual result: 11864\n",
            "3737+7915=11699\t actual result: 11652\n",
            "9438+7705=17199\t actual result: 17143\n",
            "7385+8448=15799\t actual result: 15833\n",
            "5330+387=9179\t actual result: 5717\n",
            "1354+552=6899\t actual result: 1906\n",
            "1119+8792=9899\t actual result: 9911\n",
            "6956+1282=8299\t actual result: 8238\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c73ee94",
      "metadata": {},
      "source": [
        "## Step 4.1 Train the projection layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "7ba54984",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mouad\\AppData\\Local\\Temp\\ipykernel_28604\\3849813560.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"arithmetic_modified_tuned_lens.pt\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEmbedding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-7): 8 x TransformerEncoderLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.5, inplace=False)\n",
              "      (dropout2): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = torch.load(\"arithmetic_modified_tuned_lens.pt\")\n",
        "model.eval()  # or model.train() if you want to run it in training mode for projection training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5ee853",
      "metadata": {},
      "outputs": [],
      "source": [
        "hidden_dim = 64  # same as your model's embedding dimension\n",
        "num_layers = len(model.encoder_layers)\n",
        "vocab_size=ntokens\n",
        "# Create a projection layer for each encoder layer.\n",
        "tuned_projections = nn.ModuleList([ProjectionLayer(128, ntokens) for _ in range(num_layers)])\n",
        "\n",
        "optimizer_proj = torch.optim.Adam(tuned_projections.parameters(), lr=1e-3)\n",
        "\n",
        "def train_epoch_proj():\n",
        "    # Set projection layers in training mode\n",
        "    tuned_projections.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    # Iterate over your training data as before:\n",
        "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "        # Get your batch (prompts, target_answers, etc.)\n",
        "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
        "        prompts = prompts.to(device)         # shape: [length_prompts, batch_size]\n",
        "        target_answers = target_answers.to(device)  # shape: [length_answers, batch_size]\n",
        "        # Concatenate prompts and answers as in your training loop\n",
        "        input_tensor = torch.cat((prompts, target_answers), 0)  # shape: [length_prompts + length_answers, batch_size]\n",
        "        \n",
        "        # Forward pass: get only hidden states (ignore final output)\n",
        "        # (Assumes your modified model returns (output, hidden_states))\n",
        "        _, hidden_states = model(input_tensor)\n",
        "        \n",
        "        # Compute loss using projections from each encoder layer\n",
        "        loss = 0\n",
        "        for layer_idx, hidden in enumerate(hidden_states):\n",
        "            # Each hidden has shape [seq_length, batch_size, model.ninp]\n",
        "            proj_logits = tuned_projections[layer_idx](hidden)  # shape: [seq_length, batch_size, ntoken]\n",
        "            # Select the same token positions as in your original training:\n",
        "            # Here we use tokens from position (length_prompts-1) to the one before the end.\n",
        "            proj_output_answers = proj_logits[length_prompts-1:-1, :, :].reshape(-1, ntokens)\n",
        "            loss += F.cross_entropy(proj_output_answers, target_answers.view(-1))\n",
        "        \n",
        "        optimizer_proj.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_proj.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| [Projection] {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f}'.format(\n",
        "                batch, len(data_train) // batch_size,\n",
        "                elapsed * 1000 / log_interval, cur_loss))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def train_proj():\n",
        "    num_epochs_proj = 5  # Set your projection training epochs\n",
        "    for epoch in range(1, num_epochs_proj+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_epoch_proj()\n",
        "        print('-' * 89)\n",
        "        print('| [Projection] end of epoch {:3d} | time: {:5.2f}s'.format(\n",
        "            epoch, (time.time() - epoch_start_time)))\n",
        "        print('-' * 89)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce7c2b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| [Projection]   200/ 3600 batches | ms/batch 365.38 | loss 12.98\n",
            "| [Projection]   400/ 3600 batches | ms/batch 870.95 | loss 11.39\n",
            "| [Projection]   600/ 3600 batches | ms/batch 309.75 | loss 11.23\n",
            "| [Projection]   800/ 3600 batches | ms/batch 808.97 | loss 11.15\n",
            "| [Projection]  1000/ 3600 batches | ms/batch 707.16 | loss 11.10\n",
            "| [Projection]  1200/ 3600 batches | ms/batch 959.11 | loss 11.06\n",
            "| [Projection]  1400/ 3600 batches | ms/batch 791.62 | loss 11.03\n",
            "| [Projection]  1600/ 3600 batches | ms/batch 878.64 | loss 11.00\n",
            "| [Projection]  1800/ 3600 batches | ms/batch 952.63 | loss 10.98\n",
            "| [Projection]  2000/ 3600 batches | ms/batch 897.75 | loss 10.96\n",
            "| [Projection]  2200/ 3600 batches | ms/batch 949.30 | loss 10.94\n",
            "| [Projection]  2400/ 3600 batches | ms/batch 968.26 | loss 10.93\n",
            "| [Projection]  2600/ 3600 batches | ms/batch 968.53 | loss 10.91\n",
            "| [Projection]  2800/ 3600 batches | ms/batch 709.85 | loss 10.90\n",
            "| [Projection]  3000/ 3600 batches | ms/batch 931.63 | loss 10.89\n",
            "| [Projection]  3200/ 3600 batches | ms/batch 973.31 | loss 10.88\n",
            "| [Projection]  3400/ 3600 batches | ms/batch 1053.76 | loss 10.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection] end of epoch   1 | time: 2986.91s\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection]   200/ 3600 batches | ms/batch 274.45 | loss 10.91\n",
            "| [Projection]   400/ 3600 batches | ms/batch 702.11 | loss 10.85\n",
            "| [Projection]   600/ 3600 batches | ms/batch 449.04 | loss 10.83\n",
            "| [Projection]   800/ 3600 batches | ms/batch 398.55 | loss 10.83\n",
            "| [Projection]  1000/ 3600 batches | ms/batch 426.50 | loss 10.82\n",
            "| [Projection]  1200/ 3600 batches | ms/batch 186.59 | loss 10.82\n",
            "| [Projection]  1400/ 3600 batches | ms/batch 180.29 | loss 10.81\n",
            "| [Projection]  1600/ 3600 batches | ms/batch 291.78 | loss 10.80\n",
            "| [Projection]  1800/ 3600 batches | ms/batch 230.96 | loss 10.80\n",
            "| [Projection]  2000/ 3600 batches | ms/batch 207.30 | loss 10.79\n",
            "| [Projection]  2200/ 3600 batches | ms/batch 228.52 | loss 10.79\n",
            "| [Projection]  2400/ 3600 batches | ms/batch 218.92 | loss 10.79\n",
            "| [Projection]  2600/ 3600 batches | ms/batch 965.39 | loss 10.78\n",
            "| [Projection]  2800/ 3600 batches | ms/batch 1021.69 | loss 10.78\n",
            "| [Projection]  3000/ 3600 batches | ms/batch 954.37 | loss 10.77\n",
            "| [Projection]  3200/ 3600 batches | ms/batch 966.97 | loss 10.77\n",
            "| [Projection]  3400/ 3600 batches | ms/batch 535.29 | loss 10.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection] end of epoch   2 | time: 1738.27s\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection]   200/ 3600 batches | ms/batch 462.94 | loss 10.81\n",
            "| [Projection]   400/ 3600 batches | ms/batch 430.09 | loss 10.76\n",
            "| [Projection]   600/ 3600 batches | ms/batch 455.10 | loss 10.75\n",
            "| [Projection]   800/ 3600 batches | ms/batch 442.70 | loss 10.75\n",
            "| [Projection]  1000/ 3600 batches | ms/batch 224.17 | loss 10.74\n",
            "| [Projection]  1200/ 3600 batches | ms/batch 282.82 | loss 10.74\n",
            "| [Projection]  1400/ 3600 batches | ms/batch 191.21 | loss 10.74\n",
            "| [Projection]  1600/ 3600 batches | ms/batch 223.73 | loss 10.73\n",
            "| [Projection]  1800/ 3600 batches | ms/batch 469.01 | loss 10.73\n",
            "| [Projection]  2000/ 3600 batches | ms/batch 480.60 | loss 10.73\n",
            "| [Projection]  2200/ 3600 batches | ms/batch 441.93 | loss 10.73\n",
            "| [Projection]  2400/ 3600 batches | ms/batch 421.93 | loss 10.73\n",
            "| [Projection]  2600/ 3600 batches | ms/batch 435.86 | loss 10.72\n",
            "| [Projection]  2800/ 3600 batches | ms/batch 350.75 | loss 10.72\n",
            "| [Projection]  3000/ 3600 batches | ms/batch 375.38 | loss 10.72\n",
            "| [Projection]  3200/ 3600 batches | ms/batch 282.71 | loss 10.72\n",
            "| [Projection]  3400/ 3600 batches | ms/batch 196.79 | loss 10.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection] end of epoch   3 | time: 1278.85s\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection]   200/ 3600 batches | ms/batch 380.26 | loss 10.76\n",
            "| [Projection]   400/ 3600 batches | ms/batch 188.68 | loss 10.71\n",
            "| [Projection]   600/ 3600 batches | ms/batch 311.02 | loss 10.70\n",
            "| [Projection]   800/ 3600 batches | ms/batch 446.71 | loss 10.70\n",
            "| [Projection]  1000/ 3600 batches | ms/batch 709.50 | loss 10.70\n",
            "| [Projection]  1200/ 3600 batches | ms/batch 458.43 | loss 10.70\n",
            "| [Projection]  1400/ 3600 batches | ms/batch 335.82 | loss 10.70\n",
            "| [Projection]  1600/ 3600 batches | ms/batch 216.23 | loss 10.69\n",
            "| [Projection]  1800/ 3600 batches | ms/batch 302.80 | loss 10.69\n",
            "| [Projection]  2000/ 3600 batches | ms/batch 347.60 | loss 10.69\n",
            "| [Projection]  2200/ 3600 batches | ms/batch 255.01 | loss 10.69\n",
            "| [Projection]  2400/ 3600 batches | ms/batch 359.34 | loss 10.69\n",
            "| [Projection]  2600/ 3600 batches | ms/batch 435.51 | loss 10.69\n",
            "| [Projection]  2800/ 3600 batches | ms/batch 321.80 | loss 10.69\n",
            "| [Projection]  3000/ 3600 batches | ms/batch 338.75 | loss 10.69\n",
            "| [Projection]  3200/ 3600 batches | ms/batch 427.63 | loss 10.69\n",
            "| [Projection]  3400/ 3600 batches | ms/batch 437.70 | loss 10.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection] end of epoch   4 | time: 1342.25s\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection]   200/ 3600 batches | ms/batch 447.28 | loss 10.73\n",
            "| [Projection]   400/ 3600 batches | ms/batch 464.86 | loss 10.68\n",
            "| [Projection]   600/ 3600 batches | ms/batch 438.84 | loss 10.67\n",
            "| [Projection]   800/ 3600 batches | ms/batch 433.00 | loss 10.67\n",
            "| [Projection]  1000/ 3600 batches | ms/batch 426.22 | loss 10.67\n",
            "| [Projection]  1200/ 3600 batches | ms/batch 373.33 | loss 10.67\n",
            "| [Projection]  1400/ 3600 batches | ms/batch 381.52 | loss 10.67\n",
            "| [Projection]  1600/ 3600 batches | ms/batch 515.15 | loss 10.67\n",
            "| [Projection]  1800/ 3600 batches | ms/batch 428.14 | loss 10.67\n",
            "| [Projection]  2000/ 3600 batches | ms/batch 335.66 | loss 10.67\n",
            "| [Projection]  2200/ 3600 batches | ms/batch 190.50 | loss 10.66\n",
            "| [Projection]  2400/ 3600 batches | ms/batch 184.56 | loss 10.67\n",
            "| [Projection]  2600/ 3600 batches | ms/batch 187.25 | loss 10.66\n",
            "| [Projection]  2800/ 3600 batches | ms/batch 196.45 | loss 10.66\n",
            "| [Projection]  3000/ 3600 batches | ms/batch 187.36 | loss 10.66\n",
            "| [Projection]  3200/ 3600 batches | ms/batch 368.53 | loss 10.66\n",
            "| [Projection]  3400/ 3600 batches | ms/batch 595.54 | loss 10.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| [Projection] end of epoch   5 | time: 1321.32s\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train_proj()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "e008a8db",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[103], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mplot_projection_similarity_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[103], line 12\u001b[0m, in \u001b[0;36mplot_projection_similarity_heatmap\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         similarities[i, j] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(weight_i, weight_j, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m\"\u001b[39m, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xticklabels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), yticklabels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_projection_similarity_heatmap():\n",
        "    num_layers = len(tuned_projections)\n",
        "    similarities = torch.zeros((num_layers, num_layers))\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_layers):\n",
        "            weight_i = tuned_projections[i].proj.weight.view(-1)\n",
        "            weight_j = tuned_projections[j].proj.weight.view(-1)\n",
        "            similarities[i, j] = F.cosine_similarity(weight_i, weight_j, dim=0)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(similarities.numpy(), cmap=\"coolwarm\", annot=True, xticklabels=range(1, num_layers+1), yticklabels=range(1, num_layers+1))\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.title(\"Projection Layer Similarity\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_projection_similarity_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "11c794e1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def visualize_tuned_lens(test_input, target_sequence, target_position=5):\n",
        "    \"\"\"\n",
        "    test_input: Tensor of shape [seq_length, batch_size] (transposed if necessary)\n",
        "    target_sequence: Tensor of shape [seq_length, batch_size]\n",
        "    target_position: the token index (within the sequence) to inspect.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tuned_projections.eval()\n",
        "    \n",
        "    # Forward pass through the model to get hidden states.\n",
        "    # test_input should match your model's expected input shape.\n",
        "    _, hidden_states = model(test_input)\n",
        "    \n",
        "    # Get the target token id at the specified position.\n",
        "    # Here we assume a batch size of 1 for simplicity.\n",
        "    target_token = target_sequence[target_position, 0].item()\n",
        "    \n",
        "    layer_probs = []\n",
        "    \n",
        "    # For each layer's hidden state, compute the projection logits and then the softmax.\n",
        "    for i, hidden in enumerate(hidden_states):\n",
        "        # hidden: [seq_length, batch_size, model.ninp]\n",
        "        proj_logits = tuned_projections[i](hidden)  # shape: [seq_length, batch_size, ntoken]\n",
        "        # Compute softmax to get probabilities.\n",
        "        probs = F.softmax(proj_logits, dim=-1)\n",
        "        # Extract probability for the target token at the chosen position.\n",
        "        prob = probs[target_position, 0, target_token].item()\n",
        "        layer_probs.append(prob)\n",
        "    \n",
        "    # Plot the evolution of the probability across layers.\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(1, len(layer_probs) + 1), layer_probs, marker='o')\n",
        "    plt.xlabel(\"Encoder Layer\")\n",
        "    plt.ylabel(f\"Probability of token '{target_token}'\")\n",
        "    plt.title(\"Tuned Lens: Evolution of Target Token Probability Across Layers\")\n",
        "    plt.xticks(range(1, len(layer_probs) + 1))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Suppose you fetch a test batch using your get_batch method:\n",
        "prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", 0)\n",
        "# Concatenate as during training:\n",
        "input_tensor = torch.cat((prompts, target_answers), 0).to(device)\n",
        "# Transpose if your model expects [seq_length, batch_size]:\n",
        "input_tensor = input_tensor  # assuming it's already in the proper shape, otherwise: input_tensor = input_tensor.t()\n",
        "\n",
        "# Similarly, prepare target_sequence.\n",
        "target_sequence = target_answers.to(device)\n",
        "target_sequence = target_sequence  # adjust if necessary\n",
        "\n",
        "visualize_tuned_lens(input_tensor, target_sequence, target_position=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "5722e1ff",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[102], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mplot_hidden_activation_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[102], line 4\u001b[0m, in \u001b[0;36mplot_hidden_activation_heatmap\u001b[1;34m(test_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_hidden_activation_heatmap\u001b[39m(test_input):\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 4\u001b[0m     _, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hidden_states)\n\u001b[0;32m      7\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m test_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[71], line 31\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# src: [sequence_length, batch_size] assumed\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_square_subsequent_mask(\u001b[38;5;28mlen\u001b[39m(src))\u001b[38;5;241m.\u001b[39mto(\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     32\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb(src) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mninp)\n\u001b[0;32m     33\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'device'"
          ]
        }
      ],
      "source": [
        "def plot_hidden_activation_heatmap(test_input):\n",
        "    model.eval()\n",
        "    \n",
        "    _, hidden_states = model(test_input)\n",
        "    \n",
        "    num_layers = len(hidden_states)\n",
        "    seq_length = test_input.shape[0]\n",
        "    \n",
        "    activation_heatmap = torch.zeros(seq_length, num_layers)\n",
        "    \n",
        "    for layer_idx, hidden in enumerate(hidden_states):\n",
        "        activation_heatmap[:, layer_idx] = torch.norm(hidden[:, 0, :], dim=-1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(activation_heatmap.numpy(), cmap=\"coolwarm\", xticklabels=range(1, num_layers+1), yticklabels=range(seq_length))\n",
        "    plt.xlabel(\"Transformer Layer\")\n",
        "    plt.ylabel(\"Token Position\")\n",
        "    plt.title(\"Hidden State Activation Magnitude\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_hidden_activation_heatmap(input_tensor)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qJ9IOZu8Xo4Y"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
