{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227a3167-3b01-4a1d-b838-92419ce9a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for tuned lens wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "from transformer_with_hidden import *\n",
    "from lens import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce8f10-d10a-4414-9483-984ab13271c9",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1dedc37-28e8-4500-b908-2dc8abe332e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(logits: torch.Tensor, \n",
    "                          tokenizer, \n",
    "                          top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Visualize top-k predictions from logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: The logits tensor\n",
    "        tokenizer: The tokenizer for converting token IDs to strings\n",
    "        top_k: Number of top predictions to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (token, probability) tuples for the top-k predictions\n",
    "    \"\"\"\n",
    "    # Get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    values, indices = torch.topk(probs, top_k)\n",
    "    indices, values = indices.flatten().tolist(), values.flatten().tolist()\n",
    "    \n",
    "    # Convert to list of (token, probability) tuples\n",
    "    predictions = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        probability = values[i]\n",
    "        predictions.append((token, probability))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5582afc7-f7ae-4046-afc7-76bdbc2de6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_tuned_lens(wrapper, tokenizer, input_text: str):\n",
    "    \"\"\"\n",
    "    Demonstrate the tuned lens by visualizing predictions for all intermediate layers.\n",
    "    \n",
    "    Args:\n",
    "        wrapper: The transformer model with trained tuned lens\n",
    "        tokenizer: The tokenizer\n",
    "        input_text: The input text to process\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = torch.tensor(tokenizer.encode(input_text)).view((-1,1))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = wrapper(inputs)\n",
    "    \n",
    "    # Get tuned lens outputs\n",
    "    tuned_lens_outputs = outputs['tuned_lens_outputs']\n",
    "\n",
    "    # Visualize predictions for all layers and the final prediction\n",
    "    for i, layer_outputs in enumerate(tuned_lens_outputs):\n",
    "        predictions = visualize_predictions(layer_outputs[-1, :, :].squeeze(1), tokenizer)\n",
    "        print(f\"Layer {i+1} predictions:\")\n",
    "        for token, prob in predictions:\n",
    "            print(f\"  {token}: {prob:.4f}\")\n",
    "            \n",
    "    predictions = visualize_predictions(outputs['output'][-1, :, :].squeeze(1), tokenizer)\n",
    "    print(f\"Final prediction:\")\n",
    "    for token, prob in predictions:\n",
    "        print(f\"  {token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\"\n",
    "\n",
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "id": "QuCc6jF5F8hK"
   },
   "outputs": [],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519009be-e531-4564-aa35-435f6eadd8ae",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbe0cf8-0697-475d-be1b-d044f82472db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_digits = 3\n",
    "\n",
    "# dataset_size = 64_000\n",
    "dataset_size = 640\n",
    "train_proportion = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c20ae1d-cf01-4a4c-805a-ef962d5f0023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('804+334=', '1138')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(num_digits = 3):\n",
    "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    a_str = \"\".join([str(x) for x in a_list])\n",
    "    b_str = \"\".join([str(x) for x in b_list])\n",
    "    sum_int = a_int + b_int\n",
    "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d670310-0974-4a3d-8bf0-e9148a5a4a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('396+233=', '629'),\n",
       " ('159+389=', '548'),\n",
       " ('844+433=', '1277'),\n",
       " ('222+077=', '299')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(num_digits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6230d02f-0c1a-4b32-892e-2004c13fc01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25901baf-0b60-4e7a-8f8c-6c76ae3c72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
    "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
    "    # (prompt_length, batch_size * num_samples)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
    "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
    "        if mode == \"greedy\":\n",
    "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        else: # mode == \"sampling\"\n",
    "            logits /= temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413d11a2-54ad-4875-85fd-1a151a3ce001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99a48bb-7717-4f81-a862-01c11b96ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, i, batch_size):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "\n",
    "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
    "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
    "\n",
    "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
    "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
    "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
    "\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, prompt_length, answers_length, prompts, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767319e2-d8ed-4bf9-bf05-1995a04dccae",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7911d949-8ba9-471b-af0c-f696f2cd7257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd9fbe12-e3e1-4c43-b4ce-0ee155605f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModelWithHidden(\n",
       "  (encoder): TransformerEncoderWithHidden(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (input_emb): Embedding(14, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('arithmetic.pt', weights_only=False, map_location='cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4d8e99-6257-4976-85df-88b1d2eae01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translators.0.linear.weight\n",
      "translators.0.linear.bias\n",
      "translators.1.linear.weight\n",
      "translators.1.linear.bias\n",
      "translators.2.linear.weight\n",
      "translators.2.linear.bias\n",
      "translators.3.linear.weight\n",
      "translators.3.linear.bias\n",
      "translators.4.linear.weight\n",
      "translators.4.linear.bias\n",
      "translators.5.linear.weight\n",
      "translators.5.linear.bias\n",
      "translators.6.linear.weight\n",
      "translators.6.linear.bias\n",
      "translators.7.linear.weight\n",
      "translators.7.linear.bias\n"
     ]
    }
   ],
   "source": [
    "wrapper = TransformerWithLens(model, num_layers=len(model.encoder.layers), hidden_size=model.ninp, use_tuned_lens=True)\n",
    "for name, param in wrapper.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b95c786b-9ea0-41cc-8bfb-f3091c60b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 8e-4\n",
    "\n",
    "reporting_per_epoch = 5\n",
    "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
    "assert(log_interval % batch_size == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c5cc74-de22-4876-b157-276c71d7c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch_size = batch_size):\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    wrapper.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
    "            output = wrapper(input_tensor)\n",
    "            reference = output['output'][prompt_length-1:-1,:,:] # we are only predicting the 5 last tokens\n",
    "            reference = F.log_softmax(reference, dim=-1) # KLDivLoss requires that the reference is a log probability distribution\n",
    "            tuned_lens_output = [tuned_lens_outputi[prompt_length-1:-1,:,:] for tuned_lens_outputi in output['tuned_lens_outputs']]\n",
    "            predictions = [F.softmax(tuned_lens_outputi, dim=-1) for tuned_lens_outputi in tuned_lens_output]\n",
    "            loss = torch.tensor(0.)\n",
    "            for prediction in predictions:\n",
    "                loss += F.kl_div(reference, prediction, reduction=\"batchmean\") # we sum the KL loss of each layer\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        loss = total_loss / len(data_test)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d494a250-6f91-462d-977f-62085f8c10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wrapper.train()\n",
    "    optimizer = torch.optim.AdamW(wrapper.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_test_loss = None\n",
    "    test_loss = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test loss {:5.2f}'.format(test_loss))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
    "            wrapper.zero_grad()\n",
    "            output = wrapper(input_tensor)\n",
    "            reference = output['output'][prompt_length-1:-1,:,:] # we are only predicting the 5 last tokens\n",
    "            reference = F.log_softmax(reference, dim=-1) # KLDivLoss requires that the reference is a log probability distribution\n",
    "            tuned_lens_output = [tuned_lens_outputi[prompt_length-1:-1,:,:] for tuned_lens_outputi in output['tuned_lens_outputs']]\n",
    "            predictions = [F.softmax(tuned_lens_outputi, dim=-1) for tuned_lens_outputi in tuned_lens_output]\n",
    "\n",
    "            loss = torch.tensor(0.)\n",
    "            for prediction in predictions:\n",
    "                loss += F.kl_div(reference, prediction, reduction=\"batchmean\") # we sum the KL loss of each layer\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "        test_loss = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_loss))\n",
    "        print('-' * 89)\n",
    "        # Save the tuned lens if the loss is the best we've seen so far.\n",
    "        if not best_test_loss or test_loss < best_test_loss:\n",
    "            with open(\"tuned_lens.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a0280c9-626f-4f43-828e-60d633528b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test loss 40.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "|     6/   36 batches | ms/batch 22.75 | loss 36.17 | perplexity 5104053792621019.00\n",
      "|    12/   36 batches | ms/batch 15.78 | loss 16.85 | perplexity 20757847.48\n",
      "|    18/   36 batches | ms/batch 15.56 | loss 11.05 | perplexity 63150.80\n",
      "|    24/   36 batches | ms/batch 12.39 | loss  7.81 | perplexity  2468.13\n",
      "|    30/   36 batches | ms/batch 15.66 | loss  6.51 | perplexity   672.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 10.99s | test loss  5.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "|     6/   36 batches | ms/batch 37.57 | loss  6.19 | perplexity   488.80\n",
      "|    12/   36 batches | ms/batch 19.23 | loss  4.85 | perplexity   128.10\n",
      "|    18/   36 batches | ms/batch 13.48 | loss  4.49 | perplexity    88.82\n",
      "|    24/   36 batches | ms/batch 20.92 | loss  4.48 | perplexity    87.85\n",
      "|    30/   36 batches | ms/batch 26.64 | loss  4.22 | perplexity    67.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.30s | test loss  4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|     6/   36 batches | ms/batch 13.62 | loss  4.92 | perplexity   137.17\n",
      "|    12/   36 batches | ms/batch 27.28 | loss  3.90 | perplexity    49.19\n",
      "|    18/   36 batches | ms/batch 22.97 | loss  3.76 | perplexity    43.14\n",
      "|    24/   36 batches | ms/batch 33.17 | loss  3.83 | perplexity    45.95\n",
      "|    30/   36 batches | ms/batch 11.41 | loss  3.74 | perplexity    42.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.61s | test loss  3.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "|     6/   36 batches | ms/batch  9.43 | loss  4.42 | perplexity    83.27\n",
      "|    12/   36 batches | ms/batch 15.54 | loss  3.55 | perplexity    34.94\n",
      "|    18/   36 batches | ms/batch 11.58 | loss  3.44 | perplexity    31.14\n",
      "|    24/   36 batches | ms/batch 19.71 | loss  3.53 | perplexity    34.27\n",
      "|    30/   36 batches | ms/batch 15.61 | loss  3.49 | perplexity    32.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  8.91s | test loss  3.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "|     6/   36 batches | ms/batch 30.48 | loss  4.16 | perplexity    63.85\n",
      "|    12/   36 batches | ms/batch 31.71 | loss  3.36 | perplexity    28.76\n",
      "|    18/   36 batches | ms/batch 15.66 | loss  3.25 | perplexity    25.78\n",
      "|    24/   36 batches | ms/batch 16.67 | loss  3.35 | perplexity    28.48\n",
      "|    30/   36 batches | ms/batch 24.95 | loss  3.33 | perplexity    27.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.80s | test loss  3.37\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6869d4-41e8-4eb6-b9a9-5f3fc996bb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db8d1a5f-ae92-4634-9cc8-21fd43e1d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 predictions:\n",
      "  1: 0.5138\n",
      "  9: 0.1041\n",
      "  5: 0.0836\n",
      "  4: 0.0674\n",
      "  6: 0.0557\n",
      "Layer 2 predictions:\n",
      "  5: 0.1901\n",
      "  6: 0.1870\n",
      "  4: 0.1230\n",
      "  7: 0.0945\n",
      "  9: 0.0906\n",
      "Layer 3 predictions:\n",
      "  5: 0.1832\n",
      "  6: 0.1734\n",
      "  4: 0.1355\n",
      "  3: 0.1244\n",
      "  7: 0.0976\n",
      "Layer 4 predictions:\n",
      "  4: 0.3923\n",
      "  3: 0.2919\n",
      "  2: 0.1240\n",
      "  5: 0.0996\n",
      "  1: 0.0267\n",
      "Layer 5 predictions:\n",
      "  3: 0.4145\n",
      "  4: 0.3078\n",
      "  2: 0.1869\n",
      "  5: 0.0404\n",
      "  1: 0.0261\n",
      "Layer 6 predictions:\n",
      "  2: 0.4336\n",
      "  3: 0.4282\n",
      "  4: 0.0915\n",
      "  1: 0.0309\n",
      "  5: 0.0042\n",
      "Layer 7 predictions:\n",
      "  2: 0.4812\n",
      "  3: 0.4499\n",
      "  4: 0.0363\n",
      "  1: 0.0241\n",
      "  9: 0.0026\n",
      "Layer 8 predictions:\n",
      "  2: 0.5147\n",
      "  3: 0.4605\n",
      "  1: 0.0110\n",
      "  4: 0.0090\n",
      "  [EOS]: 0.0012\n",
      "Final prediction:\n",
      "  3: 0.4946\n",
      "  2: 0.4900\n",
      "  1: 0.0082\n",
      "  4: 0.0055\n",
      "  9: 0.0005\n"
     ]
    }
   ],
   "source": [
    "demonstrate_tuned_lens(wrapper, tokenizer, input_text='123+182=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968339f0-890b-493e-b83a-5054507e43cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
